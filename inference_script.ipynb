{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero Shot Voice Conversion\n",
    "\n",
    "## Import libraries and load pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded encoder \"pretrained.pt\" trained to step 1564501\n",
      "Loaded encoder \"encoder_accent.pt\" trained to step 90001\n",
      "Found synthesizer \"Accetron_train_parallel\" trained to step 204001\n",
      "Found synthesizer \"translator_train\" trained to step 294001\n",
      "Building Wave-RNN\n",
      "Trainable Parameters: 4.481M\n",
      "Loading model weights at /home/grads/q/quamer.waris/projects/Accentron/pretrained_model/pretrained/vocoder/saved_models/pretrained/pretrained.pt\n"
     ]
    }
   ],
   "source": [
    "from synthesizer.inference import Synthesizer\n",
    "from synthesizer.kaldi_interface import KaldiInterface\n",
    "from encoder import inference as encoder\n",
    "from vocoder import inference as vocoder\n",
    "import numpy as np\n",
    "import librosa\n",
    "from pathlib import Path\n",
    "from utils.argutils import print_args\n",
    "import random\n",
    "import IPython.display as ipd\n",
    "from synthesizer.hparams import hparams\n",
    "\n",
    "\n",
    "encoder_speaker_weights = Path(\"/home/grads/q/quamer.waris/projects/Accentron/pretrained_model/pretrained/encoder/saved_models/pretrained.pt\")\n",
    "vocoder_weights = Path(\"/home/grads/q/quamer.waris/projects/Accentron/pretrained_model/pretrained/vocoder/saved_models/pretrained/pretrained.pt\")\n",
    "syn_dir = Path(\"/mnt/data1/waris/model_outputs/accentron/parallel/logs-Accetron_train_parallel/taco_pretrained\")\n",
    "\n",
    "encoder.load_model(encoder_speaker_weights)\n",
    "synthesizer = Synthesizer(syn_dir)\n",
    "vocoder.load_model(vocoder_weights)\n",
    "#hparams = hparams.parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize(bnf, embed):\n",
    "    spec = synthesizer.synthesize_spectrograms([bnf], [embed])[0]\n",
    "    generated_wav = vocoder.infer_waveform(spec)\n",
    "    generated_wav = np.pad(generated_wav, (0, synthesizer.sample_rate), mode=\"constant\")\n",
    "    return generated_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_speaker_embed(tgt_utterance_path):\n",
    "    wav, _ = librosa.load(tgt_utterance_path, hparams.sample_rate)\n",
    "    wav = encoder.preprocess_wav(wav)\n",
    "    embed_speaker = encoder.embed_utterance(wav)\n",
    "\n",
    "    return embed_speaker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate BNF for L1 reference utterance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "src_speaker = 'BDL'\n",
    "utterance_id = 'arctic_b0539'\n",
    "kaldi_dir = '/mnt/data1/waris/datasets/data/arctic_dataset/all_data/BDL/kaldi' #Path to kaldi directory of the speaker.\n",
    "ki = KaldiInterface(wav_scp=str(os.path.join(kaldi_dir, 'wav.scp')),\n",
    "                    bnf_scp=str(os.path.join(kaldi_dir, 'bnf/feats.scp')))\n",
    "bnf = ki.get_feature('_'.join([src_speaker, utterance_id]), 'bnf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tgt_utterance_path = '/mnt/data1/waris/datasets/UEDIN_mandarin_bi_data_2010/downsampled_22kHz/Mandarin_mini_testset/MF1_ENG_0001_1.wav'\n",
    "\n",
    "embed_speaker = generate_speaker_embed(tgt_utterance_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthesis_wav = synthesize(bnf, embed_speaker)\n",
    "ipd.Audio(synthesis_wav, rate=hparams.sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.io import wavfile\n",
    "\n",
    "output_dir = '/home/grads/q/quamer.waris/projects/ac-vc/synthesis_output/parallel_report_xxx'\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "output_file = os.path.join(output_dir, 'man_man_f1_222.wav')\n",
    "wavfile.write(output_file, hparams.sample_rate, synthesis_wav)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "11fd1a126a9d23a1622e8d2e6b4646d7e8bce1cbb7cad26eee91850269d70351"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('autospeech': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
